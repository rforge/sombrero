# Using Self-Organizing Maps with ```stochSOM``` to cluster a numeric dataset

#### Laura BENDHA√èBA, Madalina OLTEANU, Nathalie VILLA-VIALANEIX

## Basic package description

To be able to run the SOM algorithm, you have to load the package called 
```stochSOM```. The function used to run it is called ```trainSOM()``` and is 
detailed below.

__This documentation only considers the case of numerical data.__

```{r loading}
library(stochSOM)
```

### Arguments

The ```trainSOM``` function has several arguments, but only the first one is
required. This argument is ```x.data``` which is the dataset used to train the 
SOM. In this documentation, it is passed to the function as a matrix or a data
frame with numeric variables in columns and observations of these variables in
rows.

The other arguments are the same than the arguments passed to the ```initSOM```
function (they are parameters defining the algorithm, see ```help(initSOM)```
for further details).

### Outputs

The ```trainSOM``` function returns an object of class ```somRes``` (see 
```help(trainSOM)``` for further details on this class).

## First case study: the iris dataset

This first case study is performed on the famous (Fisher's or Anderson's) iris
data set that gives the measurements in centimeters of the variables sepal
length and width and petal length and width, respectively, for 50 flowers from
each of 3 species of iris (setosa, versicolor, and virginica).

### Training the SOM

The first four variables of the data set (that are the numeric variables) are used to map the
each flower on the SOM grid.
```{r irisTrain, cache=TRUE}
set.seed(4031730)
# the following instruction run the SOM algorithm with verbose set to TRUE
iris.som <- trainSOM(x.data=iris[,1:4],
                    verbose=TRUE, nb.save=5)
iris.som
```

As the energy is registered during the intermediate backups, we can have a look
at its evolution.
```{r energyIris}
plot(iris.som, what="energy")
```
which is stabilized during the last 100 iterations.

### Resulting clustering

The clustering component contains the final classification of the dataset. It is 
a vector having a length equal to the number of rows of the input dataset.
```{r irisClusters}
iris.som$clustering
table(iris.som$clustering)
```
which can also be visualized by a hitmap plot:
```{r irisHitmap}
plot(iris.som, what="obs", type="hitmap")
```

To access the relevance of each explanatory variable in the definition of the 
clusters, the function ```summary``` includes an ANOVA with the factor being the
clustering, for each (numeric) input variables.
```{r irisSummary}
summary(iris.som)
```
Here, all variables have significantly different means among the different
clusters and can thus be considered to be relevant for the clustering
definition.

### Clustering interpretation

The clusters can be interpreted by representing the values of the different
prototypes on the map:
```{r irisPrototypes}
par(mfrow=c(2,2))
plot(iris.som, what="prototypes", var="Sepal.Length", main="Sepal length")
plot(iris.som, what="prototypes", var="Sepal.Width", main="Sepal width")
plot(iris.som, what="prototypes", var="Petal.Length", main="Petal length")
plot(iris.som, what="prototypes", var="Petal.Width", main="Petal width")
```
The clusters located at the bottom of the grid contain flowers with large and
long petals and long sepals, whereas flowers with large sepals are clustered on
the left hand side of the grid.

## Second case study: simulated data in $latex \mathbb{R}^2$

The second case study show the clustering of points randomly distributed in the
square $latex [0,1]^2$. The data are generated by:
```{r dataGeneration}
set.seed(4031719)
the.data <- data.frame("x1"=runif(500), "x2"=runif(500))
plot(the.data, pch=19)
```

### Training the SOM
The numeric SOM algorithm is used to cluster the data:
```{r dataTrain, cache=TRUE}
set.seed(4031731)
# run the SOM algorithm with 10 intermediate backups and 2000 iterations
my.som <- trainSOM(x.data=the.data, dimension=c(5,5), nb.save=10, maxit=2000,
                   scale="none")
```

The energy evolves as described in the following graphic:
```{r energy}
plot(my.som, what="energy")
```
It is stabilized during the last 500 iterations.

### Clustering

The resulting clustering distribution can be visualized by the hitmap:
```{r hitmapObs}
plot(my.som, what="obs", type="hitmap")
```
The observations are almost uniformly distributed on the map.

The clustering component allows us to plot the initial data according to the 
final clustering.

```{r clusteredData, echo=FALSE}
# prepare a vector of colors
my.colors <- rainbow(prod(my.som$parameters$the.grid$dim))[my.som$clustering]

# points depicted with the same color are in the same final cluster
plot(my.som$data[,1], my.som$data[,2], col=my.colors, pch=19, xlab="x1", 
     ylab="x2", main="Data according to final clustering")
```

### Clustering interpretation

The values of the prototypes can be represented with the plot function and help
interpret the clusters:
```{r colorProto, fig.width=5, fig.height=2.5}
par(mfrow=c(1,2))
plot(my.som, what="prototypes", type="color", var=1, main="prototypes - x1")
plot(my.som, what="prototypes", type="color", var=2, main="prototypes - x2")
```
Here, the interpretation is simple enough: high values of the first variables x1
are located at the bottom of the map and small values at the top of the map.
Large values of x2 are located at the right hand side of the map, whereas, small
values are located at the left hand side of the map.

The prototypes coordinates are also registered for each intermediate backup so 
they can be displayed on different graphics to see the evolution in the
prototypes organization.
```{r protoEvoluation, fig.width=15, fig.height=6, echo=FALSE}
# find out the prototypes to be linked
tra <- NULL
for (i in c(1,6,11,16,21)){
  tra <- c(tra, i, i+1, i+1, i+2, i+2, i+3, i+3, i+4)
}
for (i in c(1:5)){
  tra <- c(tra, i, i+5, i+5, i+10, i+10, i+15, i+15, i+20)
}
tmp <- matrix(tra, ncol=2, byrow=TRUE)
# plot the prototypes
par(mfrow=c(2, 5),mar=c(3,2,2,1))
invisible(sapply(1:my.som$parameters$nb.save, function(ind){
  plot(my.som$backup$prototypes[[ind]][,1], my.som$backup$prototypes[[ind]][,2],
       xlab="", ylab="", main=c("iteration ", my.som$backup$steps[ind]))
  for (i in 1:nrow(tmp)){
    segments(x0=my.som$backup$prototypes[[ind]][tmp[i,1],1], 
             y0=my.som$backup$prototypes[[ind]][tmp[i,1],2],
             x1=my.som$backup$prototypes[[ind]][tmp[i,2],1], 
             y1=my.som$backup$prototypes[[ind]][tmp[i,2],2], 
             col="red", pch=19)
  }
}))
```
At the begining of the algorithm, the prototypes are randomly distributed in
$latex \mathbb{R}^2$ and then, they organize as a regular rectangular grid in
$latex [0,1]^2$.